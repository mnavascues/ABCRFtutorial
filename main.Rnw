\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{url}
\usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue,bookmarks=true,bookmarksopen=true,bookmarksopenlevel=3,pdfstartview=FitH,pdfview=FitH]{hyperref}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage[]{algorithm}
\usepackage[first=1, last=10, seed=1234]{lcg}
\usepackage{ifthen}
\usepackage[nomarkers,nofiglist,figuresonly]{endfloat}
\usepackage[acronym]{glossaries}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{Guide to ABC}
\rhead{M. Navascués}
\rfoot{\thepage}
\lfoot{\href{http://doi.org/10.5281/zenodo.1435503}{doi:10.5281/zenodo.1435503}}


\newif\iflatexboolean


\makeglossaries

\newglossaryentry{reftable}
{   name=reference table,
    description={data matrix that contains for each simulations the value of the parameters and summary statistics}
}
\newacronym{abc}{ABC}{approximate Bayesian computation}






\bibliographystyle{abbrvnat}

\title{A Beginners Guide to Approximate Bayesian Computation in Population Genetics}

\author[1,2,*]{Miguel Navascués}
\affil[1]{INRA, UMR CBGP, F-34988 Montferrier-sur-Lez, France}
\affil[2]{Institut de Biologie Computationnelle, F-34090 Montpellier, France}
\affil[*]{miguel.navascues@inra.fr}
\date {\today}

\begin{document}

\maketitle
\newpage


%\LaTeX{} version: \LaTeXe~\fmtversion



<<echo=FALSE, cache=FALSE>>=
read_chunk("src/main.r")
@

<<rcode_0, cache=FALSE, tidy=TRUE, echo=FALSE>>=
@

\section*{Introduction}

Work in progress...

\section*{Classical ABC}

\subsection*{Likelihood}

\paragraph*{\href{http://en.wikipedia.org/wiki/Bernoulli_trial}{The experiment}:} A coin is tossed 10 times and the results (heads/tails, or head/tree in our case) are recorded

\paragraph*{The result\footnote{Images by \href{http://commons.wikimedia.org/wiki/File:LIBAN,_5_piastres.jpg}{CBG Numismatique Paris}, CC-BY-SA}:} 


<<cache=FALSE, echo=FALSE>>=

toss_LaTeX <- array("true",10)
toss_LaTeX[which(toss=="H")] <- 'false'

@


\begin{center}
\latexboolean\Sexpr{toss_LaTeX[1]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[2]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[3]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[4]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[5]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[6]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[7]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[8]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[9]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi
\latexboolean\Sexpr{toss_LaTeX[10]}
\iflatexboolean
  \includegraphics[height=1.1cm]{data/Tails.jpg}
\else
  \includegraphics[height=1.1cm]{data/Heads.jpg}
\fi

\end{center}


<<rcode_1, cache=FALSE, tidy=TRUE>>=
@

\noindent Number of tosses (\verb+total_tosses+): \Sexpr{total_tosses}\\
Number of heads (\verb+heads_count+): \Sexpr{heads_count}\\
Number of tails (\verb+tails_count+): \Sexpr{tails_count}

\paragraph*{The question:} Is the coin fair? = Is the probability of getting ``heads'' fifty percent?

\[ L(p=0.5|D) = P(D|p=0.5) \]

\noindent where $p$ is the probability of getting ``heads'' in a coin toss, $L$ is the likelihood and $D$ is the data (\Sexpr{heads_count} heads and \Sexpr{tails_count} tails).

<<rcode_2, cache=FALSE, tidy=TRUE>>=
@

\noindent Probability of combination \Sexpr{toss}: \Sexpr{p_combo}.\\

That is the prbability for a given order of \Sexpr{heads_count} heads in 10 coin tosses. All ordered combinations of \Sexpr{heads_count} heads and \Sexpr{tails_count} tails have the same probability. The number of combination is given by the \href{http://en.wikipedia.org/wiki/Binomial_distribution}{binomial coefficient} $\binom{n}{k}=\frac{n!}{k!(n-k)!}$  ({\tt \Sexpr{hi_latex('choose(n,k)')}} in {\tt R}).

<<rcode_3, cache=FALSE, tidy=TRUE>>=
@

\noindent There are \Sexpr{combinations} combinations of \Sexpr{heads_count} head and \Sexpr{tails_count} tails.

<<rcode_4, cache=FALSE, tidy=TRUE>>=
@

\noindent Likelihood of $p=\Sexpr{p_heads}$ given \Sexpr{heads_count} heads in \Sexpr{total_tosses} tosses: \Sexpr{likelihood}.\\

In the case of the coin flip experiment the likelihood can be calculated from a binomial probability model (assuming coin tosses are independent and have the same probability of getting heads). We can load file {\tt flip\_coin\_likelihood.r} that contains some functions to calculate this likelihood (i.e.~the code lines presented above wraped in a function: {\tt \Sexpr{hi_latex('flip.coin.likelihood(n,k,p)')}}:


<<rcode_5, cache=FALSE, tidy=TRUE>>=
@



A \href{http://en.wikipedia.org/wiki/Maximum_likelihood_estimation}{maximum likelihood estimate} and \href{http://en.wikipedia.org/wiki/Confidence_interval#Methods_of_derivation}{confidence intervals} can be obtained from the likelihood profile:

<<rcode_5b, cache=FALSE, tidy=TRUE>>=
@

The maximum likelihood estimate is $\hat{p}=\Sexpr{p_hat}$ with a 95\% confidence interval of $(\Sexpr{CI95})$.


\paragraph{Exercise 1:} Make flip-coin experiments (or simulate them in {\tt R} with {\tt \Sexpr{hi_latex('sample()')}}) with different number of tosses and observe how confidence intervals change.
% <<Exercise1, echo=F, hide=T, cache=F>>=
% @

\noindent Code for figure~\ref{fig:plotLikelihood}:
<<plotLikelihood, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Likelihood profile for the flip coin experiment. Continous orange line indicates maximum likelihood value and dashed orange lines show 95\\%CI.">>=
plot(x    = likelihood_profile$p,
     y    = likelihood_profile$likelihood,
     xlab = "p",
     ylab = "Likelihood",
     type = "l")
abline(v = p_hat, col = 2, lwd = 2)
abline(h = exp(log(maxL)-1.92), col = 6, lwd = 2)
abline(v = CI95[1], lty = 2, col = 2, lwd = 2)
abline(v = CI95[2], lty = 2, col = 2, lwd = 2)
@

\subsection*{A is for Approximation (1 of 3: Simulation)}

Lets assume that our experiment is decribed by a model that does not have an analytical solution. Likelihood can be estimated with Monte Carlo methods. In this context a simulation is the production of data from a probabilistic model using \href{http://en.wikipedia.org/wiki/Pseudorandom_number_generator}{(pseudo)random numbers} to decide the outcome. In the case of the flip coin experiment, we can simulate flipping the coin by using the binomial distribution:

<<rcode_6, cache=FALSE, tidy=TRUE>>=
@

\noindent Simulated toss: \Sexpr{toss_simulation}.

Many simulations can be performed to estimate the probability of a given outcome (e.g. \Sexpr{heads_count} heads) as the proportion of simulations with the same ourcome. The function {\tt \Sexpr{hi_latex('flip.coin.likelihood.approx(n,k,p,rep)')}} will simulate {\tt rep} coin tosses to estimate the likelihood for parameter value {\tt p} (function from file {\tt flip\_coin\_likelihood.r}).

<<rcode_7, cache=FALSE, tidy=TRUE>>=
@

\noindent Estimated likelihood of $p=\Sexpr{p_heads}$ given \Sexpr{heads_count} heads in \Sexpr{total_tosses} tosses \Sexpr{likelihood_approx$likelihood} (true value: \Sexpr{likelihood}).

\paragraph{Exercise 2:} Use function {\tt \Sexpr{hi_latex('flip.coin.likelihood.approx(n,k,p,rep)')}} with options {\tt \Sexpr{hi_latex('rep=10')}} and {\tt \Sexpr{hi_latex('trace=TRUE')}} to visualize some simulated coin tosses. Have a look at the code of the function.\\ 
% <<Exercise2, echo=F, hide=T, cache=F>>=
% @

Repeating for many values of parameter $p$ we can estimate the likelihood profile:
<<rcode_8, cache=FALSE, tidy=TRUE>>=
@

\noindent Code for figure~\ref{fig:plotLikelihoodHat}:
<<plotLikelihoodHat, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Estimated likelihood profile (orange) for a flip coin experiment by simulation. Black line shows the targeted likelihood profile.">>=
plot(x    = likelihood_profile$p,
     y    = likelihood_profile$likelihood,
     xlab = expression(italic(p)), ylab = "Likelihood",
     lwd  = 2, type = "l")
lines(x   = likelihood_profile_approx$p ,
      y   = likelihood_profile_approx$likelihood,
      col = 2, lwd = 2)
@

\paragraph{Exercise 3:} Estimate the likelihood profile for the observed coin toss using different number of simulations to evaluate the effect in the accuracy of the estimate.
% <<Exercise3, echo=F, hide=T, cache=F>>=
% @


\subsection*{B is for Bayesian\footnote{But see \citet{Rousset2017}}}

The posterior probability is the probability of the parameter $p$ given the data $D$: $P(p|D)$. Using Bayes' Theorem: $P(p|D)=\frac{P(D|p)P(p)}{P(D)}$, where $P(D|p)=L(p|D)$ is the likelihood and $P(p)$ is the prior probability. The marginal likelihood, $P(D)$, is constant and does not need to be calculated: $P(p|D)\propto L(p|D)P(p)$.

We already know how to estimate the likelihood from simulations for a given value of $p$ ($L(p=\Sexpr{p_heads}|D=\Sexpr{heads_count}\mathrm{H})\approx \Sexpr{likelihood_approx$likelihood}$, as estimated above).

Lets try to estimate the prior probability with the same strategy as the likelihood. The previous knowledge or beliefs about the parameter are expressed in the form of the prior probability distribution. For the moment we are going to assume that all possible values of $p$ have the same probability, that is, a uniform distribution $p\sim\mathrm{U}(0,1)$. We can take values from that distribution and count how many times we get the $p=\Sexpr{p_heads}$):
<<rcode_9a, cache=FALSE, tidy=TRUE>>=
@

A proportion of \Sexpr{prior_p_approx} draws had exactly the value $p=\Sexpr{p_heads}$. Indeed, the prior probability for a given value of $p$ is $0$, $P(p=\Sexpr{p_heads})=0$; remember that a probability distribution gives you the probability \textbf{density}. We need to change a litttle bit the strategy, using the rejection algorithm: 

\begin{algorithm}[H]
  \label{alg:exact_rejection}
	\caption{Exact rejection sampler}
	\begin{algorithmic}
	 \STATE Given $N$ the number of simulations
		\FOR {$i=1$ to $N$} 
			 \STATE Generate $p' \sim \pi(p)$
			 \STATE Simulate $D' \sim f(x|p')$
       \IF{$D'=D$} \STATE Accept $p'$ \ENDIF
		\ENDFOR
	 \RETURN accepted $p'$
	\end{algorithmic}
\end{algorithm}

In {\tt R}:

<<rcode_9b, cache=FALSE, tidy=TRUE>>=
@

The values of $p$ from the simulations kept after applying the rejection algorithm are used to estimate the posterior probability distribution. In this simple case (coin flipping experiment) we did not need to estimate it is known when using a conjugate prior such as the beta distribution, $\mathrm{B}(\alpha,\beta)$. Thus, the posterior distribution is $\mathrm{B}(\alpha+\mathrm{\#heads},\beta+\mathrm{\#tails})$, in our case $\mathrm{B}(1+\Sexpr{heads_count},1+\Sexpr{tails_count})$. Note that the uniform distribution $\mathrm{U}(0,1)$ is the same as beta distribution $\mathrm{B}(1,1)$. 

\noindent Code for figure~\ref{fig:abc_rejection_coin}:
<<abc_rejection_coin, fig.height=10, fig.width=6, echo=TRUE, fig.cap="Simulations of coin tosses. Paramter $p$ taken from a uniform distribution.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(x    = sim_parameter_p1,
     y    = sim_data_p1,
     xlab = expression(italic(p)*"'"),
     ylab = expression(italic(D)*"'"))
abline(h = heads_count, col = 7)
points(sim_parameter_kept_p1, sim_data_kept_p1, col = 7)

hist(x      = sim_parameter_p1,
     breaks = seq(0,1,0.02),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,5),
     main   = "",
     xlab   = expression(italic(p)),
     ylab   = "probability density")
hist(x      = sim_parameter_kept_p1, 
     breaks = seq(0,1,0.02), 
     col    = Vermillion_transparency,
     freq   = FALSE,
     add    = TRUE); box()
lines(x   = seq(0,1,0.001),
      y   = dbeta(x=seq(0,1,0.001),1+heads_count,1+tails_count),
      col = 7,
      lwd = 3)
@



From the prior probability distribution we can obtain a point estimates (usually the median) and 95\% highest posterior denstity: 

<<rcode_10, cache=FALSE, tidy=TRUE>>=
@

Point estimate of parameter is $\hat{p}=\Sexpr{round(p_hat,2)}$ with (\Sexpr{round(as.vector(p_95CI),2)}) 95\% credibility interval.



\paragraph{Exercise 4:} Using a beta distribution as a prior, re-analyse the data with a prior believe that the coin is fair: We have examinated the coin before the experiment and we did not notice anything abnormal; in addition we have read \citet{Diaconis2007}.
% <<Exercise4, echo=F, hide=T, cache=F>>=
% @



\subsection*{C is \textit{not} for Coalescent}

<<rcode_13, echo=F, hide=T, cache=F>>=
@

In order to advance we leave our toy example of the coin toss and continue with a population genetics example. We are going to study two fictitious datasets consisting in resequencing experiments of \Sexpr{sample_size} gene copies of a non recombining locus (e.g. a mitochondrial gene). The data is in {\tt ms} format  \citep{Hudson2002} in folder {\tt data}, files {\tt dataset1.txt} and {\tt dataset2.txt}.

We can have a look at the data. We will use {\tt Sampling} \citep[file {\tt ms.r}][with some modifcications by myself]{Stadler2009a,Stadler2009b}, which is a {\tt R} script with functions to read files in {\tt ms} format and calculate summary statistics (number of segregating sites, {\tt S}; nucleotide diversity, {\tt pi}; number of haplotypes, {\tt NH}; site frequency spectrum, {\tt SFS}; Tajima's D, {\tt TajimasD}; Fay and Wu H, {\tt FayWuH}; Fu and Li D, {\tt FuLiD}):

<<rcode_14_A, cache=F, tidy=TRUE>>=
@

Dataset 1 has a sample size of \Sexpr{sample_size} gene copies, with \Sexpr{target1_S} polymorphic sites and \Sexpr{target1_NH} different haplotypes. The site frequency spectrum from this sample is represented in figure~\ref{fig:SFS_1}:
<<SFS_1, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Site frequensy spectrum Dataset 1.">>=
colnames(target1_SFS)<-1:(sample_size-1)
barplot(height = target1_SFS/target1_S,
        main   = "Unfolded Site Frequency Spectrum",
        xlab   = "derived allele count in sample",
        ylab   = "Proportion of sites")
box()
@


\paragraph{Exercise 5:} Calculate summary statistics for Dataset 2. What are the main differences with dataset 1?\\
<<Exercise5, echo=F, results=F, cache=F, fig.show='hide'>>=
@

The generating model for such data is the coalescent. We will be using a version of {\tt ms} coalescent simulator \citep{Hudson2002} implemented in the R package {\tt phyclust} \citep{Chen2011}. This is a choice to make the present tutorial available to different operative systems (i.e. Windows). %However, in my experience, {\tt ms} implementation in {\tt phyclust} suffers some problems and there seem to be bugs for complex demographies. For real data analysis I use the original {\tt ms} software in a Linux environment (see box).

\subsection*{A is for Approximation (2 of 3: Summary Statistics)}

We have, thus, a new tyope of data (DNA sequences) and its corresponding data simulator (the coalescent). We could try to apply the same rejection algorithm described above, but the structure of the data is more complex and an exact match would occur rarely in simulations. So we use the second approximation of ABC, instead of the data we use summary statistics ($S$) from the data to classify the simulation as a match to the observation. It is said now that we are approximating the likelihood of $p$ given the summary statistic, $L(p|S)$, rather than the likelihood of $p$ given the data, $L(p|D)$.

\begin{algorithm}[H]
  \label{alg:exact_rejection_SS}
	\caption{Exact rejection sampler on summary statistics}
	\begin{algorithmic}
	 \STATE Calculate $S$ from $D$
	 \STATE Given $N$ the number of simulations
		\FOR {$i=1$ to $N$} 
			 \STATE Generate $p' \sim \pi(p)$
			 \STATE Simulate $D' \sim f(x|p')$
			 \STATE Calculate $S'$ from $D'$
       \IF{$S'=S$} \STATE Accept $p'$ \ENDIF
		\ENDFOR
	 \RETURN accepted $p'$
	\end{algorithmic}
\end{algorithm}


<<rcode_15, cache=F, tidy=TRUE>>=
@

<<rcode_15_b, cache=F, echo=F, results=F, tidy=TRUE, warning=FALSE>>=
@

We can apply this algorthim to our Dataset 1, simulating with the coalescent simulator and using the number of segregating sites (number of polymorphisms) as summary statistic:

<<rcode_16, cache=F, tidy=TRUE>>=
@

\noindent Code for figure~\ref{fig:abc_rejection_coal}:
<<abc_rejection_coal, fig.height=10, fig.width=6, echo=TRUE, fig.cap="Coalescent simulations. Paramter $\\theta$ is taken from a log-uniform distribution.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot( x    = log10(sim_theta),
      y    = sim_SS,
      xlab = expression(log[10](theta*"'")),
      ylab = "SS'",
      ylim = c(0,100))
abline(h = target_SS, col = 7)
points(log10(sim_theta_kept), sim_SS_kept, col = 7)

hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,6),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density")
hist(x      = log10(sim_theta_kept),
     breaks = seq(-1,2,0.05), 
     col    = Vermillion_transparency,
     freq   = FALSE,
     add    = TRUE); box()
@

The number of simulations kept is \Sexpr{length(sim_SS_kept)} out of \Sexpr{length(sim_SS)}.\\

\paragraph{Exercise 6:} Apply \acrshort{abc} rejection algorithm to Dataset 2 using a different summary statistic, such as the nucletide diversity,  $\pi$ ({\tt pi}). Repeat using several summary statistics.\\
% <<Exercise6, echo=F, hide=T, cache=F>>=
% @

\subsection*{A is for Approximation (3 of 3: Tolerance)}

Summarizing the data into statistics is not enough (in practice) to allow a big enough number of simulations to match the observed data. The third approximation in ABC allows to accept simulations that are not an exact match but are in the close neighbourhood of the observation.

\begin{algorithm}[H]
  \label{alg:rejection_SS}
	\caption{Exact rejection sampler on summary statistics}
	\begin{algorithmic}
	 \STATE Calculate $S$ from $D$
	 \STATE Given $N$ the number of simulations
		\FOR {$i=1$ to $N$} 
			 \STATE Generate $p' \sim \pi(p)$
			 \STATE Simulate $D' \sim f(x|p')$
			 \STATE Calculate $S'$ from $D'$
       \IF{$\mathrm{distance}(S',S)<\delta$} \STATE Accept $p'$ \ENDIF
		\ENDFOR
	 \RETURN accepted $p'$
	\end{algorithmic}
\end{algorithm}

From this point we are goint to start using functions from the {\tt R} package {\tt abc} to present a more compact code:

<<rcode_17, cache=F, tidy=TRUE>>=
@

\noindent Code for figure~\ref{fig:abc_rejection_tolerance}:
<<abc_rejection_tolerance, fig.height=10, fig.width=6, echo=TRUE, fig.cap="Coalescent simulations. Paramter $\\theta$ is taken from a log-uniform distribution.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(x    = log10(sim_theta),
     y    = sim_SS,
     xlab = expression(log[10](theta*"'")),
     ylab = "SS'",
     ylim = c(0,50))
abline(h = target_SS, col = 7)
abline(h = max(abc_result$ss), col = 7, lty = 2)
abline(h = min(abc_result$ss), col = 7, lty = 2)
points(log10(sim_theta_kept_tol1), sim_SS_kept_tol1, col = 7)

hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,4),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density")
hist(x      = log10(sim_theta_kept_tol1),
     breaks = seq(-1,2,0.05),
     col    = Vermillion_transparency, 
     freq   = FALSE,
     add    = TRUE); box()
@

The number of simulations kept is \Sexpr{length(sim_theta_kept_tol1)} out of \Sexpr{dim(sim_SS)[1]}; determined by tolerance of \Sexpr{tolerance}.\\


\paragraph{Exercise 7:} Reanalyse the data using different tolerance values. What happens when we get tolerance values close to 1?\\
% <<Exercise7, echo=F, hide=T, cache=F>>=
% @

\subsection*{Regression}

Introduced by \citet{Beaumont2002}


<<rcode_18, cache=F, tidy=TRUE>>=
@

\noindent Code for figure~\ref{fig:abc_regression}:
<<abc_regression, fig.height=10, fig.width=6, echo=TRUE, fig.cap="Coalescent simulations. Paramter $\\theta$ is taken from a log-uniform distribution.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(x    = log10(sim_theta),
     y    = sim_SS,
     xlab = expression(log[10](theta*"'")),
     ylab = "SS'",
     ylim = c(0,30))
abline(h = target_SS, col = 6)
{abline(h = max(abc_result$ss), col = 6, lty = 2)
 abline(h = min(abc_result$ss), col = 6, lty = 2)}
points(log10(sim_theta_kept), sim_SS_kept, col = 6)
abline(a   = -local_regression$coefficients[1]/local_regression$coefficients[2],
       b   = 1/local_regression$coefficients[2],
       col = 5,
       lwd = 3)
points(x   = log10(sim_theta_kept)[1:10],
       y   = sim_SS_kept[1:10],
       col = 5)
arrows(x0     = log10(sim_theta_kept)[1:10],
       y0     = sim_SS_kept[1:10],
       x1     = log10(sim_theta_adjusted)[1:10],
       y1     = target_SS,
       col    = 5,
       length = 0.1)
hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,4),
     main   = "",
     xlab   = expression(log(theta)),
     ylab   = "probability density")
hist(x      = log10(sim_theta_kept),
     breaks = seq(-1,2,0.05),
     col    = Blue_transparency, freq=FALSE, add=TRUE )
wtd.hist(x      = log10(sim_theta_adjusted),
         breaks = seq(-1.5,2,0.05),
         col    = Yellow_transparency, freq=FALSE, add=TRUE,
         weight = abc_result$weights); box()
@



\subsection*{Information on Summary Statistics}

Now that we have determined that using the rejection plus regression approach is the way to go we can apply it to estimate $\theta$:

<<rcode_19, cache=F, tidy=TRUE>>=
@

With a very unsatisfactory result (figure~\ref{fig:hist_coal4}).
<<hist_coal4, fig.pos="p!", fig.height=6, fig.width=6, echo=TRUE, fig.cap="Probability distribution. Grey histogram: prior probability distribution.">>=
hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,4),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density")
wtd.hist(x      = log10(sim_theta_adjusted_D),
         breaks = seq(-2,3,0.05),
         col    = BluishGreen_transparency,
         freq   = FALSE, add = TRUE,
         weight = sim_weights_D); box()
@

\paragraph{Exercise 8:} Make a scatterplot for simulated values of $\theta$ and Tajima's D. Reanalyse the data using a summary statistics that is informative for parameter $\theta$.\\
% <<Exercise8, echo=F, hide=T, cache=F>>=
% @

For a quick and dirty evaluation of the informativeness of these summary statistics with the parameter $\theta$ we can look at the correlation coeficient:

<<rcode_20, cache=F, tidy=TRUE>>=
@

Note, however, that Tajima's D might not be very informative for $\theta$ on its own, but it is infromative in combination with other summary statistics:

<<rcode_21, cache=F, tidy=TRUE>>=
@

Which gives us a nicer result (figure~\ref{fig:hist_coal5}).
<<hist_coal5, fig.pos="p!", fig.height=6, fig.width=6, echo=TRUE, fig.cap="Probability distribution. Grey histogram: prior probability distribution.">>=
hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,4),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density")
wtd.hist(x      = log10(sim_theta_adjusted_piD),
         breaks = seq(-2,3,0.05),
         col    = BluishGreen_transparency,
         freq   = FALSE, add = TRUE,
         weight = sim_weights_piD); box()
@

However, adding summary statistics can make worse the results, when the summary statistic is completely random respect to the parameter:

<<rcode_22, cache=F, tidy=TRUE>>=
@

\noindent Code for figure~\ref{fig:hist_coal6}:
<<hist_coal6, fig.pos="p!", fig.height=6, fig.width=6, echo=TRUE, fig.cap="Probability distribution. Grey histogram: prior probability distribution.">>=
hist(x      = log10(sim_theta),
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,4),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density")
wtd.hist(x      = log10(sim_theta_adjusted_pi_noise), 
         breaks = seq(-2,2,0.05), 
         col    = ReddishPurple_transparency, 
         freq   = FALSE, add = TRUE, 
         weight = sim_weights_pi_noise); box()
@


\subsection*{Beware of the Prior}

Priors, as discussed above, can be used to incroporate previous information on the parameters. In the case of our population genetics example, we could imagine to have some information on mutation rates (e.g. from pedigrees) and population sizes (from census). Since $\theta=4N_e\mu$, we could draw parameters $N_e$ and $\mu$ from prior probability distributions and combine them to get the prior of $\theta$:

<<rcode_23, cache=F, tidy=TRUE>>=
@
Which will give you a prior for $\theta$ with a peak, as in figure~\ref{fig:prior}:
<<prior, fig.pos="p!", fig.height=6, fig.width=6, echo=TRUE, fig.cap="Probability distribution. Grey histogram: prior probability distribution.">>=
hist(x      = log10(sim_theta_composite),
     breaks = seq(-2,3,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,1),
     xlim   = c(-1,2),
     main   = "",
     xlab   = expression(log[10](theta)),
     ylab   = "probability density"); box()
@

ALWAYS COMPARE POSTERIOR AND PRIOR PROBABILITY DISTRIBUTIONS!!!


\paragraph{Exercise 9:} Estimate $\theta$ using appropiate ABC algorithm, prior and summary statistics for Dataset 1 and 2.\\
<<Exercise9, echo=F, cache=F, fig.show='hide'>>=
@

\subsection*{Quality Control 1: Cross Validation}

The package {\tt abc} includes a function for cross validation:
<<rcode_24, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:CV}:
<<CV, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Cross validation.">>=
plot(x    = log10(cross_validation_result$true$theta),
     y    = log10(cross_validation_result$estim$tol0.1),
     xlab = expression(log(theta)),
     ylab = expression(log(hat(theta))),
     xlim = c(-1,2), ylim = c(-1,2))
abline(a = 0, b = 1)
@



\subsection*{Quality Control 2: Goodness of fit (prior)}

Are simulations from the model and priors producing data similar to observed data? 

figure~\ref{fig:GoodnessFit1}:
<<GoodnessFit1, fig.height=8, fig.width=6, echo=TRUE, fig.cap="Goodness of fit.">>=
par(mfrow=c(3,1),mar=c(4.2,4.2,1,1))
plot(sim1_S,sim1_pi,xlab="S",ylab=expression(pi),log="xy")
points(target1_S,target1_pi,col=7,cex=4,pch="*")
points(target2_S,target2_pi,col=6,cex=4,pch="*")

plot(sim1_NH,sim1_pi,xlab="Number of Haplotypes",ylab=expression(pi),log="xy")
points(target1_NH,target1_pi,col=7,cex=4,pch="*")
points(target2_NH,target2_pi,col=6,cex=4,pch="*")

plot(sim1_FuLiD,sim1_pi,xlab="Fu and Li's D",ylab=expression(pi),log="y",xlim=c(-6,2))
points(target1_FuLiD,target1_pi,col=7,cex=4,pch="*")
points(target2_FuLiD,target2_pi,col=6,cex=4,pch="*")
par(mfrow=c(1,1),mar=c(5.1,4.1,4.1,2.1))
@

\subsection*{Quality Control 3: Goodness of fit (posterior)}

We can also verify the goodness of fit of the inferred parameters. We can sample from the posterior distribution by using the regression-adjusted values of the retained simulations. We can re-simulate data from those values to obtain the expected distribution of summary statistics under the posterior and compare it with the observation:
<<rcode_25, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:GoodnessFit2}:
<<GoodnessFit2, fig.height=8, fig.width=6, echo=TRUE, fig.cap="Goodness of fit. Dataset 1.">>=
par(mfrow=c(2,1))
hist(x      = posterior_NH, 
     breaks = seq(0,40,1),
     col    = "grey",
     freq   = FALSE,
     xlab   = "Number of haplotypes", main = "")
abline(v = target1_NH, col = 7, lwd = 2); box()
hist(x      = posterior_FuLiD,
     breaks = seq(-5,4,0.2),
     col    = "grey",
     freq   = FALSE, 
     xlab   = "Fu and Li's D", main = "")
abline(v = target1_FuLiD, col = 7, lwd = 2); box()
@

\paragraph{Exercise 10:} Control goodness of fit of posterior probability distribution for Dataset 2.\\
% <<Exercise10, echo=F, results=F, cache=F, fig.show='hide'>>=
% @

\subsection*{Model Choice}

We are going to consider two models. A constant size model with parameter $\theta=4N_e\mu$ and a population with one instantaneous change of size with parameters $\theta_0=4N_{e0}\mu$ (present scaled mutation rate), $\tau=t\mu$ (mutation scaled time of population size change) and $\theta_1=4N_{e1}\mu$ (past scaled mutation rate). We already have simulations for the first model, we can add the simulations for the second model.

<<rcode_26, cache=F, tidy=TRUE>>=
@

<<rcode_26_b, echo=F, cache=F, tidy=TRUE>>=
@

Quality control: goodness of fit for model + prior:

figure~\ref{fig:GoodnessFit3}:
<<GoodnessFit3, fig.height=8, fig.width=6, echo=TRUE, fig.cap="Goodness of fit.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(sim2_S,sim2_pi,xlab="S",ylab=expression(pi),log="xy")
points(sim1_S,sim1_pi,col="grey")
points(target1_S,target1_pi,col=7,cex=3,pch="*")
points(target2_S,target2_pi,col=6,cex=3,pch="*")

plot(sim2_FuLiD,sim2_pi,xlab="Fu and Li's D",ylab=expression(pi),log="y",xlim=c(-6,2))
points(sim1_FuLiD,sim1_pi,col="grey")
points(target1_FuLiD,target1_pi,col=7,cex=3,pch="*")
points(target2_FuLiD,target2_pi,col=6,cex=3,pch="*")
@




We put together the two \gls{reftable}s.
<<rcode_27, cache=F, tidy=TRUE>>=
@

And we performed an ABC rejection+regression on the model. The main differences is that it is a logistic regression because the model is a qualitative variable:

<<rcode_28, cache=F, tidy=TRUE>>=
@

The best model for Dataset 1 is \Sexpr{names(which(abc_model_choice1$pred==max(abc_model_choice1$pred)))} (C for constant, V for varable population size), with a posterior probability of \Sexpr{max(abc_model_choice1$pred)}

The best model for Dataset 2 is \Sexpr{names(which(abc_model_choice2$pred==max(abc_model_choice2$pred)))} (C for constant, V for varable population size), with a posterior probability of \Sexpr{max(abc_model_choice2$pred)}



\section*{ABC random forest}


\subsection*{One tree: classification and regression tree}

A decission tree is a simple machine learning model for target quantitative (regression)  or qualitative (classification) variables: 

<<rcode_29, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:CARTregression}:
<<CARTregression, fig.height=8, fig.width=6, echo=TRUE, fig.cap="Regression tree.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(regression_tree)
text(regression_tree,cex=0.75)

plot(ref_table$TajD,
     ref_table$pi,
     xlab="Tajima's D",
     ylab=expression(pi),
     col=cbPalette1[round(log10(theta)+3)],
     #col=grey(1-theta/max(theta)),
     pch=20)
partition.tree(regression_tree,
               ordvars=c("TajD","pi"),
               add=T,cex=1)
@

<<rcode_30, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:CARTclassification}:
<<CARTclassification, fig.height=8, fig.width=6, echo=TRUE, fig.cap="Classification tree.">>=
par(mfrow=c(2,1),mar=c(4.2,4.2,1,1))

plot(classification_tree)
text(classification_tree,cex=0.75)

plot(ref_table$TajD[which(sim_model=="V")],
     ref_table$FuLiD[which(sim_model=="V")],
     xlab="Tajima's D",
     ylab="Fu and Li's D",
     pch=20)
points(ref_table$TajD[which(sim_model=="C")],
       ref_table$FuLiD[which(sim_model=="C")],
       col="grey",pch=20)
partition.tree(classification_tree,
               ordvars=c("TajD","FuLiD"),
               add=T,cex=3,col=7)
@

\subsection*{Many trees: random forest}

<<rcode_31, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:CARTregression1}:
<<CARTregression1, fig.height=10, fig.width=6, echo=TRUE, fig.cap="Classification tree.">>=
par(mfrow=c(3,1),mar=c(4.2,4.2,1,1))
plot(regression_tree)
text(regression_tree,cex=0.75)

plot(ref_table$pi,
     log10(ref_table$theta),
     xlab=expression(pi),
     ylab=expression(log[10]*theta),
     pch=20,log="x")
partition.tree(regression_tree,
               ordvars=c("pi","theta"),
               add=T,cex=1.5,col=6,lwd=2)

plot(ref_table$pi,
     log10(ref_table$theta),
     xlab=expression(pi),
     ylab=expression(log[10]*theta),
     pch=20,log="x")
for (i in 1:100){
  random_sample <- sample(length(theta),size=300,replace=T)
  ref_table_random_sample <- ref_table[random_sample,]
  regression_tree_random_sample <- tree(log10(theta) ~ pi,
                                        data=ref_table_random_sample)
  partition.tree(regression_tree_random_sample,
                 ordvars=c("pi","theta"),
                 add=T,cex=1.5,col=7,lwd=1)
  
}
@

\subsection*{Getting posterior probabilities from RF}

As we have seen above, Random Forest allows you to classify your observation in a category (i.e. model choice) and obtain an estimate of a quantitative variable (i.e. parameter estimation). But these values are not associated to posterior probabilities. The package {\tt abcrf} \citep{Marin2017} implements the development that allow to get these probabilities using random forest.

For model choice, random forest provides you with the number of votes supporting each of the models (number of votes = number of trees leading to a given model). However, the number of votes is not necessary a good quantitative measure of the incertitude of the model choice. \Citet{Pudlo2016} developped the approach to estimate the posterior probability. First, a random forest is grown for model choice:

<<rcode_32, cache=F, tidy=TRUE>>=
@

figure~\ref{fig:VarImportance}:
<<VarImportance, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Variable importance plot.">>=
plot(model_RF,
     training=ref_table)
@


For each simulation of the reference table we can get a prediction from the random forest (only from trees grown without that simulation). Comparing with the true value we can get the prior error rate and the confusion matrix:

<<rcode_33, cache=F, tidy=TRUE>>=
@

Now we can grow a \textbf{second} random forest which will learn the relationship between the ``local'' error rate and the summary statistics. The probability of a correct classification (posterior probability) is one minus the probability of an incorrect classification (error rate). This second random forest allows to estimate the posterior probability of the \textbf{chosen} model (implemented in function {\tt \Sexpr{hi_latex('predict.abcrf()')}} from {\tt abcrf}):

<<rcode_34, cache=F, tidy=TRUE>>=
@

For the posterior probability distribution of a parameter, the key thing to remember is that prediction of quantitative variables from random forest is based on the weight given by the trees to each simulation of the reference table (i.e. the rediction is a weigthed mean). The same weigths can be applied to the median and percentiles to obtain a point estimate and credibility intervals, and to plot an estimate of the distribution (e.g. an histogram) \citep{Raynal2017}:

<<rcode_35, cache=F, tidy=TRUE>>=
@
<<rcode_36, cache=F, tidy=TRUE>>=
@


figure~\ref{fig:hist_abcrf}:
<<hist_abcrf, fig.height=6, fig.width=6, echo=TRUE, fig.cap="Probability density estimated from ABCRF.">>=
hist(x      = log10theta,
     breaks = seq(-1,2,0.05),
     col    = "grey",
     freq   = FALSE,
     ylim   = c(0,5),
     main   = "",
     xlab   = expression(log(theta)),
     ylab   = "probability density")
wtd.hist(x      = log10theta,
         breaks = seq(-1,2,0.05),
         col    = ReddishPurple_transparency, freq=FALSE, add=TRUE,
         weight = posterior_theta_RF$weights); box()
@










\section*{Some further topics}

Other ABC flavours: SMC-ABC

Simulation software: msprime \citep{Kelleher2016}, SLiM \citep{Haller2017}.

There are several reviews on abc: \citep{Csillery2010}, \citep{Bertorelle2010}

Other applications than population genetics: phylogenetics \citep{Lintusaari2016}, community ecology, epidemiology, (astronomy)


\section*{Acknowledgements}

I was initiated to the use of {\tt R} for ABC by M. Beaumont, L. Chikhi and V. Sousa at the Gulbenkian course MMPG08.

\printglossary[type=\acronymtype]
 
\printglossary

\bibliography{references}


\end{document}

